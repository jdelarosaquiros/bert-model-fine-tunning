# -*- coding: utf-8 -*-
"""Bert Model on GLUE - ALL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fseb-qtCLzU1mmqFmoBtNIR90ygLOEcy
"""

! pip install datasets transformers

!apt install git-lfs

!pip install transformers[torch]

#load all libraries
import requests
import json
import torch
import torch.nn as nn
import os
from tqdm import tqdm
from transformers import BertModel, BertTokenizer, AdamW
# AutoTokenizer, AutoModelForQuestionAnswering, BertTokenizer, BertForQuestionAnswering
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ExponentialLR
import matplotlib.pyplot as plt
from datasets import load_dataset, load_metric

"""# Fine-tuning and testing a model on a text classification tasks"""

GLUE_TASKS = ["cola", "mnli", "mrpc", "qnli", "qqp", "rte", "sst2", "stsb", "wnli"]

# Hyperparameters

model_checkpoint = "bert-base-uncased"
batch_size = 32
max_length = 128
epochs = 3
task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mnli-mm": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

# Load Models

tokenizer = BertTokenizer.from_pretrained(model_checkpoint)
bert_model = BertModel.from_pretrained(model_checkpoint)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print("Device:", device)

# Convert data into tokens: input_ids, token_type_ids, attention_mask

def preprocess_function(examples):
    if sentence2_key is None:
        return tokenizer(examples[sentence1_key], max_length = MAX_LENGTH, truncation=True, padding=True)
    return tokenizer(examples[sentence1_key], examples[sentence2_key], max_length = MAX_LENGTH, truncation=True, padding=True)

# Model Data Class for Text Classification

class InputDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, i):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][i]),
            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),
            'labels': torch.tensor(self.encodings['labels'][i]),
        }
    def __len__(self):
        return len(self.encodings['input_ids'])

# Model Class for Sequence Classification

class QAModel(nn.Module):
    def __init__(self, num_labels):
        super(QAModel, self).__init__()
        self.num_labels = num_labels

        self.bert = bert_model
        self.dropout = nn.Dropout(0.1)
        self.linear = nn.Linear(768, num_labels)
        self.classifier = nn.Sequential(
            self.dropout,
            self.linear,
        )

    def forward(self, input_ids, attention_mask, token_type_ids):
        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        pooled_output = model_output[1]
        logits = self.classifier(pooled_output)

        prediction_logits = logits.squeeze()

        return prediction_logits

def loss_fn(prediction_logits, labels, num_labels):

  if num_labels == 1:
      problem_type = "regression"
  elif num_labels > 1:
      problem_type = "single_label_classification"
  else:
      problem_type = "multi_label_classification"

  if problem_type == "regression":
      loss_fct = nn.MSELoss()
      if num_labels == 1:
          loss = loss_fct(prediction_logits, labels)
      else:
          loss = loss_fct(prediction_logits, labels)

  elif problem_type == "single_label_classification":
      loss_fct = nn.CrossEntropyLoss()
      loss = loss_fct(prediction_logits, labels)

  elif problem_type == "multi_label_classification":
      loss_fct = nn.BCEWithLogitsLoss()
      loss = loss_fct(prediction_logits, labels)

  return loss

def train_epoch(model, optimizer, scheduler, dataloader, epoch, total_acc = [], total_loss = []):
    model = model.train()
    losses = []
    acc = []
    ctr = 0
    batch_tracker = 0
    for batch in tqdm(dataloader, desc = 'Running Epoch '):
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        labels = batch['labels'].to(device)

        out_predictions = model(input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids)

        loss = loss_fn(out_predictions, labels, num_labels)

        losses.append(loss.item())
        loss.backward()
        optimizer.step()

        if task != 'stsb':
          out_predictions = torch.argmax(out_predictions, dim=1)

        acc.append(((out_predictions == labels).sum()/len(out_predictions)).item())

        batch_tracker = batch_tracker + 1
        if batch_tracker==250 and epoch==1:
            total_acc.append(sum(acc)/len(acc))
            loss_avg = sum(losses)/len(losses)
            total_loss.append(loss_avg)
            batch_tracker = 0

    scheduler.step()
    ret_acc = sum(acc)/len(acc)
    ret_loss = sum(losses)/len(losses)
    return(ret_acc, ret_loss)

def eval_model(model, dataloader):
  model = model.eval()
  losses = []
  acc = []
  ctr = 0
  with torch.no_grad():
      for batch in tqdm(dataloader, desc = 'Running Evaluation'):
          input_ids = batch['input_ids'].to(device)
          attention_mask = batch['attention_mask'].to(device)
          token_type_ids = batch['token_type_ids'].to(device)
          labels = batch['labels'].to(device)

          out_predictions = model(input_ids=input_ids,
              attention_mask=attention_mask,
              token_type_ids=token_type_ids)

          loss = loss_fn(out_predictions, labels, num_labels)

          losses.append(loss.item())

          if task != 'stsb':
            out_predictions = torch.argmax(out_predictions, dim=1)

          acc.append(((out_predictions == labels).sum()/len(out_predictions)).item())

      ret_acc = sum(acc)/len(acc)
      ret_loss = sum(losses)/len(losses)

  return(ret_acc, ret_loss)

metrics = {}

for task in GLUE_TASK:
  dataset = load_dataset("glue", task)

  # Tokenize data

  validation_key = "validation_matched" if task == "mnli" else "validation"
  test_key = "test_matched" if task == "mnli" else "test"

  training_encodings = preprocess_function(dataset['train'])
  training_encodings['labels'] = dataset['train']['label']
  validation_encodings = preprocess_function(dataset[validation_key])
  validation_encodings['labels'] = dataset[validation_key]['label']

  # Create Model

  num_labels = 3 if task.startswith("mnli") else 1 if task=="stsb" else 2
  model = QAModel(num_labels)

  model.to(device)

  # Creating Optimizer

  optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)
  scheduler = ExponentialLR(optim, gamma=0.9)


  # Initiate Model Data

  train_dataset = InputDataset(training_encodings)
  valid_dataset = InputDataset(validation_encodings)

  # Load Data

  train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
  valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)




  total_acc = []
  total_loss = []

  train_acc_per_epoch = []
  train_loss_per_epoch = []
  test_acc_per_epoch = []
  test_loss_per_epoch = []


  for epoch in range(epochs):
      train_acc, train_loss = train_epoch(model, optimizer, scheduler, train_data_loader, epoch+1, total_acc, total_loss)
      print(f"Train Accuracy: {train_acc}      Train Loss: {train_loss}")
      val_acc, val_loss = eval_model(model, valid_data_loader)
      print(f"Validation Accuracy: {val_acc}   Validation Loss: {val_loss}")

      train_acc_per_epoch.append(train_acc)
      train_loss_per_epoch.append(train_loss)
      test_acc_per_epoch.append(val_acc)
      test_loss_per_epoch.append(val_loss)

  val_acc, val_loss = eval_model(model, valid_data_loader) # This is actually valid
  print(f"Testing Accuracy: {val_acc}   Testing Loss: {val_loss}")

  metrics[task]

# Save Metrics
json_metricts = json.dumps(metrics)
with open(f'bert_glue_metricts.json', 'w') as f:
  f.write("%s" % json_metricts)

